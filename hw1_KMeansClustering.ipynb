{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq79ToiM0ieV"
      },
      "source": [
        "# Homework 1 - KMeans Clustering\n",
        "For homework 1, you will apply kmeans clustering to real environmental datasets. This assignment is designed to provide hands-on practices. \n",
        "\n",
        "MAKE YOUR OWN COPY OF THIS FILE BEFORE YOU START. \n",
        "\n",
        "Complete each task and submit your Jupyter notebook on Blackboard.\n",
        "\n",
        "# Section:\n",
        "- Random Initialization\n",
        "- Assign Points\n",
        "- Compute Centroids\n",
        "- KMeans Clustering\n",
        "- Silhouette Score\n",
        "- Real-World Applications\n",
        "  - Climate\n",
        "  - Wildfire (MS Student Only)\n",
        "\n",
        "## To-Do Lists\n",
        "Look out for sections marked \"# IMPLEMENT\" and \"# QUESTION\"\n",
        "- Undergrads: 4 Implement Blocks + 1 Question Block - 5 Points Total\n",
        "- Masters: 6 Implement Blocks + 1 Question Block - 7 Points Total\n",
        "\n",
        "Partial credits will be given.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrvLVQjRL-Uc"
      },
      "source": [
        "\n",
        "\n",
        "--- \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HK41FqatMul"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "np.random.seed(500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh_Oup5Q1mU9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('NumPy')' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n NumPy ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# randomly generated example data to cluster\n",
        "example_X = np.concatenate([np.random.multivariate_normal((3,3),((1,0),(0,1)), 30), \n",
        "                           np.random.multivariate_normal((-3,3),((1,-0.5),(-0.5,1)), 30), \n",
        "                           np.random.multivariate_normal((3,-3),((1,0),(0,1)), 30)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z22rkiJ6kc9"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.4 ('NumPy')' requires ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'conda install -n NumPy ipykernel --update-deps --force-reinstall'"
          ]
        }
      ],
      "source": [
        "print(example_X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTP4YkYF2p9H"
      },
      "outputs": [],
      "source": [
        "# example data visualization\n",
        "plt.scatter(example_X[:,0], example_X[:,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4b5PH309q3v"
      },
      "source": [
        "## [1] Random Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT397YAAz4Eg"
      },
      "outputs": [],
      "source": [
        "\"\"\" \n",
        "\n",
        "Randomly initialize cetroids for kmeans clustering based on points X.\n",
        "X is passed in as a numpy array. You should randomly choose k different\n",
        "points from X.\n",
        "\n",
        "Output should be a numpy array of size KxM where K is the number of centroids\n",
        "and M is the number of features.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def random_initialization(X: np.array, k: int):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # IMPLEMENT - 1 Point\n",
        "    # -------------------------------------------------------------------------\n",
        "    initial_centroids = X.copy()\n",
        "    np.random.shuffle(initial_centroids)\n",
        "    return initial_centroids[:k]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NkcHmYz9sqk"
      },
      "source": [
        "## [2] Assign Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHavlaUX3rwF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Given dataset X, \"cluster\" (assign) each point to be the centroid that is closest\n",
        "according to euclidean distance, you can implement euclidean distance yourself\n",
        "or use sklearn's implementation.\n",
        "\n",
        "Output should be a numpy array of shape NxM where n is the number of points and \n",
        "m is the number of features.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def assign_points(X: np.array, centroids: np.array):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # IMPLEMENT - 1 Point\n",
        "    # -------------------------------------------------------------------------\n",
        "    point_assignments = np.empty([len(X), X.shape[1]])\n",
        "    allCentroidDistances = euclidean_distances(X, centroids)\n",
        "    for i in range(len(allCentroidDistances)):\n",
        "        row = allCentroidDistances[i]\n",
        "        minVal = np.iinfo(np.int32).max\n",
        "        currentPoint = row\n",
        "        for cell in row:\n",
        "            if cell < minVal:\n",
        "                minVal = cell\n",
        "        result = np.where(currentPoint == minVal)\n",
        "        point_assignments[currentPoint] = result[0]\n",
        "    return point_assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDd8BhK19wYG"
      },
      "source": [
        "## [3] Compute Centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2MxBIvS4lHu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Given dataset X (size NxM), and cluster assignments (NxM), compute centroids of each cluster\n",
        "according to the given point assignments.\n",
        "\n",
        "Output should be a numpy array of shape (KxM) \n",
        "where K is the number of clusters, and M is the number of features.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def compute_centroids(X: np.array, point_assignments: np.array, k: int):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # IMPLEMENT - 1 Point\n",
        "    # -------------------------------------------------------------------------\n",
        "    #3*k sums... one for each running total of feature position of points corresponding to each centroid (x,y)\n",
        "    #one for the number of points corresponding to each centroid\n",
        "    #then average the feature sums for each centroid (divide by num points sum) to yield k new x,y positions \n",
        "    centroids = np.empty([k, X.shape[1]])\n",
        "    while k > 0:\n",
        "        for a in point_assignments:\n",
        "            result = np.where(a==k)\n",
        "            #ksum = sum(X[result[0]])\n",
        "            #kcount = len(result[0])\n",
        "            #kpos = ksum / kcount\n",
        "            kpos = np.mean(X[result[0]], axis=0)\n",
        "            centroids[k] = kpos\n",
        "        k=k-1\n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yxNqt_t9x3W"
      },
      "source": [
        "## [4] KMeans Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQW9l3qe5Rot"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "\n",
        "Given dataset X and number K, cluster points into K categories using K-Means clustering.\n",
        "\n",
        "To do this you will need to initialize cluster centers\n",
        "repeat until cluster centers are not changed:\n",
        "    update point assignments to clusters\n",
        "    update cluster centers\n",
        "\n",
        "This should return clusters as a numpy array (Nx1) with one entry for each point in X, \n",
        "with entries corresponding to cluster ids from 0 to K-1.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from mimetypes import init\n",
        "\n",
        "\n",
        "def KMeans_cluster(X: np.array, k: int):\n",
        "    # -------------------------------------------------------------------------\n",
        "    # IMPLEMENT - 1 Point\n",
        "    # -------------------------------------------------------------------------\n",
        "    initCentroids = random_initialization(X, k)\n",
        "    x =1\n",
        "    while(x == 1):\n",
        "        assign_points(X, initCentroids)\n",
        "        prevCentroids = initCentroids\n",
        "        initCentroids = compute_centroids(X, initCentroids, k)\n",
        "        if(prevCentroids.all() == initCentroids.all()):\n",
        "            break\n",
        "\n",
        "    final_cluster_assignments = initCentroids\n",
        "    return final_cluster_assignments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhrDqFYl6yhT"
      },
      "source": [
        "Now we can use the clustering code to do interesting things like determine homogeneous bio-regions. Let's test it first on our example dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkjv81yB6WQM"
      },
      "outputs": [],
      "source": [
        "cluster_assignments = KMeans_cluster(X = example_X, k = 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2mzQIb47AbB"
      },
      "outputs": [],
      "source": [
        "plt.scatter(example_X[:,0], example_X[:,1], c=cluster_assignments)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvT3_Xlc77US"
      },
      "source": [
        "## [5] Real-World Application - Climate\n",
        "Run all of these steps to see eco-regions from raw data.\n",
        "The raw data is geospatial data in the form of 'rasters', a grid overlay over a geographic extent where each cell is associated with one or more values. In our case each 10 by 10 meters grid cell across the world is associated with a set of environmental variables such as temperatures and precipitation. In fact, we have 3 sets of data one for the present environmental values of each grid cell, and two for future projected climates under two different climate change scenarios rcp2.6 and rcp8.5 in year 2050. This is the well known WORLDCLIM dataset (https://www.worldclim.org/data/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Liq-Vz77-UR",
        "outputId": "47e8c188-7635-4a63-8cd3-a8b9f49e56c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rasterio in /usr/local/lib/python3.7/dist-packages (1.2.10)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.4.7)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from rasterio) (2022.6.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rasterio) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rasterio) (1.21.6)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio) (7.1.2)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.7/dist-packages (from rasterio) (2.3.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from rasterio) (0.7.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio) (22.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install rasterio\n",
        "!wget -Nq http://biogeo.ucdavis.edu/data/climate/worldclim/1_4/grid/cur/bio_10m_esri.zip\n",
        "\n",
        "!wget -Nq http://biogeo.ucdavis.edu/data/climate/cmip5/10m/bc85bi50.zip\n",
        "!wget -Nq http://biogeo.ucdavis.edu/data/climate/cmip5/10m/bc26bi50.zip\n",
        "\n",
        "!unzip -nq bio_10m_esri.zip\n",
        "!unzip -nq bc85bi50.zip -d projection_data\n",
        "!unzip -nq bc26bi50.zip -d projection_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHL-SrdV8PB_"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import os             # do cross platform manipulation of filesystem\n",
        "\n",
        "import numpy as np    # matrix library\n",
        "import scipy\n",
        "import scipy.spatial  # for distance calculations\n",
        "\n",
        "import rasterio       # read in and handle rasters\n",
        "\n",
        "import matplotlib     # plotting library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns # plotting library expansion\n",
        "\n",
        "from sklearn import preprocessing  # for preprocessing and storing data transformations\n",
        "from sklearn import metrics # for quantitatively evaluating our ml models\n",
        "from scipy import stats     # statistics library for describing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tIbmN218Tbr"
      },
      "outputs": [],
      "source": [
        "# ordered list of feature names found here: http://www.worldclim.org/bioclim\n",
        "feature_names = [\n",
        "  'Annual Mean Temperature',\n",
        "  'Mean Diurnal Range (Mean of monthly (max temp - min temp))',\n",
        "  'Isothermality (BIO2/BIO7) (* 100)',\n",
        "  'Temperature Seasonality (standard deviation *100)',\n",
        "  'Max Temperature of Warmest Month',\n",
        "  'Min Temperature of Coldest Month',\n",
        "  'Temperature Annual Range (BIO5-BIO6)',\n",
        "  'Mean Temperature of Wettest Quarter',\n",
        "  'Mean Temperature of Driest Quarter',\n",
        "  'Mean Temperature of Warmest Quarter',\n",
        "  'Mean Temperature of Coldest Quarter',\n",
        "  'Annual Precipitation',\n",
        "  'Precipitation of Wettest Month',\n",
        "  'Precipitation of Driest Month',\n",
        "  'Precipitation Seasonality (Coefficient of Variation)',\n",
        "  'Precipitation of Wettest Quarter',\n",
        "  'Precipitation of Driest Quarter',\n",
        "  'Precipitation of Warmest Quarter',\n",
        "  'Precipitation of Coldest Quarter'\n",
        "]\n",
        "\n",
        "# format strings that map us to the files we want\n",
        "# using os.path.join to get platform independent paths\n",
        "current_feat_file_name_fmt = os.path.join(\"bio\",\"bio_{feat_ind}\",\"hdr.adf\")\n",
        "future_feat_file_name_fmt = os.path.join(\"projection_data\",\"{model}{feat_ind}.tif\")\n",
        "\n",
        "model_names = [\"bc26bi50\",\"bc85bi50\"]\n",
        "\n",
        "current_filenames = [current_feat_file_name_fmt.format(feat_ind=feat_ind) for feat_ind in range(1, len(feature_names)+1)]\n",
        "\n",
        "# dictionary (hashtable) mapping a model name to a list of feature names\n",
        "# we can look up the feature names for a given model name by doing\n",
        "# future_filenames[\"model\"], take a look at \n",
        "# future_filenames.keys() to see what models there are\n",
        "future_filenames = {model:[future_feat_file_name_fmt.format(model=model, feat_ind=feat_ind) for feat_ind in range(1, len(feature_names)+1)] for model in model_names}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YPE0IDr8VZg"
      },
      "outputs": [],
      "source": [
        "def load_features(file_names):\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    Load masked rasters from a list of filenames\n",
        "    Loads all the data in the given input filenames and masks away any square in the raster that does not have an observation for \"all\" of the features\n",
        "    \n",
        "    Args:\n",
        "        file_names (str list) : List of filenames from which to read individual features\n",
        "    \n",
        "    Returns:\n",
        "        data (numpy masked array) : numpy matrix excluding entries with no data in any feature\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    mask = None\n",
        "    raw_data = []\n",
        "    for file_name in file_names:    \n",
        "        # open raster for reading the raw data\n",
        "        with rasterio.open(file_name, \"r\") as f:\n",
        "            data = f.read().squeeze().astype(np.float32)\n",
        "        raw_data.append(data)\n",
        "        if mask is None:\n",
        "            mask = (data == f.nodata)\n",
        "        else:\n",
        "            mask = mask | (data == f.nodata)\n",
        "    \n",
        "    # mask away cells for which we have no data\n",
        "    data = np.ma.array(np.stack(raw_data, axis=-1),mask=np.repeat(mask[:,:,None],len(file_names), axis=-1))\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGqXI8HN8W9z"
      },
      "outputs": [],
      "source": [
        "def show_map(data, title=None):\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    Plots data for a scalar map\n",
        "    \n",
        "    Args:\n",
        "        data (numpy array) : raster of values to plot\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.imshow(data, cmap=\"Blues\")\n",
        "    plt.colorbar(fraction=0.03, pad=0.04)\n",
        "    plt.grid(\"off\")\n",
        "    if title is not None:\n",
        "        plt.title(title, fontsize=16)\n",
        "    plt.show()\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfdFFaXz8btp"
      },
      "outputs": [],
      "source": [
        "current_features = load_features(current_filenames)\n",
        "projected_features = {model: load_features(model_files) for model, model_files in future_filenames.items()}\n",
        "\n",
        "# create an empty raster with the same size and mask to plot useful things later\n",
        "empty_raster = np.ma.array(np.zeros_like(current_features[:,:,0]),mask=current_features.mask[:,:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC0OYXM98fRU"
      },
      "outputs": [],
      "source": [
        "def flatten_mask(data):\n",
        "    return data[~data.mask[:,:,0],:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00h6iekM8nLO"
      },
      "outputs": [],
      "source": [
        "current_observations = flatten_mask(current_features)\n",
        "projected_observations = {model: flatten_mask(projected_features[model]) for model, model_files in future_filenames.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEtjclsN8lDL"
      },
      "outputs": [],
      "source": [
        "print(\"current indicators shape:\",current_observations.shape)\n",
        "print(\"best case scenario shape:\",projected_observations[\"bc26bi50\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prGLiQZoDDbg"
      },
      "source": [
        "Transform and rescale features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp1BMtLm8o3m"
      },
      "outputs": [],
      "source": [
        "def log_preprocess(data, log_features=None):\n",
        "    res = data.copy()\n",
        "    if log_features is not None:\n",
        "        res[:,log_features] = np.log(res[:,log_features] + 1)\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivYs-_-ba7Dh"
      },
      "outputs": [],
      "source": [
        "def log_inv(data, log_features=None):\n",
        "    res = data.copy()\n",
        "    if log_features is not None:\n",
        "        res[:,log_features] = np.exp(res[:log_features]) - 1\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQackWWc8yoW"
      },
      "outputs": [],
      "source": [
        "log_features = None\n",
        "log_features = [11,12,13,14,15,16,17,18]\n",
        "\n",
        "log_transform = preprocessing.FunctionTransformer(func=log_preprocess,inverse_func=log_inv,kw_args={\"log_features\":log_features},check_inverse=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhYqSDxv8zFR"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler(copy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Jxc3yJh80sv"
      },
      "outputs": [],
      "source": [
        "processed_current_observations = scaler.fit_transform(log_transform.fit_transform(current_observations))\n",
        "processed_projections = {model: scaler.transform(log_transform.fit_transform(model_observations)) for model, model_observations in projected_observations.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52Bxpuq384si"
      },
      "outputs": [],
      "source": [
        "def plot_cluster(labels, title=None, empty_raster=empty_raster):\n",
        "    label_raster = empty_raster.copy()\n",
        "    label_raster[~label_raster.mask] = labels\n",
        "    plt.figure(figsize=(18,6))\n",
        "    im = plt.imshow(label_raster, cmap='tab20', vmin=0, vmax=labels.max())\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "    if title is not None:\n",
        "        plt.title(title, fontsize=16)\n",
        "    plt.grid(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2sJUqsa9ABI"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "training_data = np.concatenate([processed_current_observations, processed_projections[\"bc26bi50\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQyKF_gGDSUp"
      },
      "source": [
        "# Cluster on climate data - KMeans used here!\n",
        "\n",
        "Note: If you cannot get your own implementation of K-Means clustering to work, replace the call to your function below with calling the sklearn implementation of batched K-Means and demonstrating results for several number of clusters, and checking how regions change based on number of clusters (for some credit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OvlgdJA9CU3"
      },
      "outputs": [],
      "source": [
        "current_labels = KMeans_cluster(processed_current_observations, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrUYIBfQ9EY5"
      },
      "outputs": [],
      "source": [
        "plot_cluster(current_labels, \"clustering of current data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvURhygJu-s_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "Q: Are Canada and Europe in the same cluster?\n",
        "\n",
        "'''\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# QUESTION - 1 Point\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYJvesczDcCw"
      },
      "source": [
        "Optional Exploration: Let us below rerun the the clustering with same K (but different initializations because of the randomness), what happens to the results? Next rerun the clustering for larger K=4,6,8, and comment on what you observe with the resulting maps, what would be your recommended K based on this manual inspection?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqttpr9lDjSs"
      },
      "outputs": [],
      "source": [
        "k=4\n",
        "current_labels = KMeans_cluster(processed_current_observations, k)\n",
        "plot_cluster(current_labels, \"clustering of current data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_v2jOAODqHw"
      },
      "source": [
        "OPTIONAL: share your observations here in text comments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd1os4B2psej"
      },
      "source": [
        "##  [6] Real-World Application - Wildfire (MS Student Only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzBERLsSuqg0"
      },
      "source": [
        "The Fire Program Analysis Fire-Occurrence Database (FPA FOD) includes 1.88 million geo-referenced wildfire records from 1992 to 2015. (https://www.kaggle.com/datasets/rtatman/188-million-us-wildfires). Wildfire.csv is generated by filtering FPA FOD: (1) wildfire size class, wildfire year, and state; (2) 2001 - 2015; (3) 50 states. Extract the largest wildfire class for each year in each state and map wildfire size class from letters to numbers (ex. Class A => 0, Class B => 1, so on...). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEsN4zlVpqqo"
      },
      "outputs": [],
      "source": [
        "!wget -Nq https://raw.githubusercontent.com/csci461/dataset/main/wildfire.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFMESn0QqDYZ"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-z1aBr1uWbc"
      },
      "source": [
        "FIRE_SIZE_CLASS = Code for fire size based on the number of acres within the final fire perimeter expenditures (A=greater than 0 but less than or equal to 0.25 acres, B=0.26-9.9 acres, C=10.0-99.9 acres, D=100-299 acres, E=300 to 999 acres, F=1000 to 4999 acres, and G=5000+ acres). Map wildfire size class from letters to numbers (ex. Class A => 0, Class B => 1, so on...). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0pMRxHmsLdy"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"wildfire.csv\",index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tl204wNSeJi"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "Use sklearn KMeans (set random state to 123, use default hyperparameters) and silhouette_score\n",
        "\n",
        "Cluster 50 states by wildfire size class from 2001 to 2015\n",
        "\n",
        "Use silhouette score to determine top 2 number of clusters (M clusters, N clusters)\n",
        "\n",
        "Show line plot - number of clusters (from 2 to 19) vs silhouette score \n",
        "\n",
        "'''\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# IMPLEMENT - 1 Point\n",
        "# -------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wv3X3kjrR-g"
      },
      "outputs": [],
      "source": [
        "def cluster_map(df,cluster_label,title):\n",
        "  \n",
        "    \"\"\"\n",
        "\n",
        "    Plot cluster labels on 50 states map\n",
        "    \n",
        "    Args:\n",
        "        df (pandas dataframe) : dataframe with states as index and year as columns\n",
        "        cluster_label (numpy array) : cluster labels for 50 states\n",
        "        title (str) : title for the plot\n",
        "\n",
        "    Returns:\n",
        "        data (numpy masked array) : numpy matrix excluding entries with no data in any feature\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    df_cluster = df.copy()\n",
        "    df_cluster[\"Cluster\"] = [str(item) for item in list(cluster_label)]\n",
        "    fig = px.choropleth(df_cluster,locations = df_cluster.index, locationmode=\"USA-states\", scope=\"usa\",\n",
        "                      color=\"Cluster\",category_orders={\"Cluster\": np.sort(df_cluster[\"Cluster\"].unique())},\n",
        "                      title=title)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwei4rVltBkz"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "Cluster 50 states by wildfire size class from 2001 to 2015 with the best number of clusters M\n",
        "\n",
        "Show cluster map\n",
        "\n",
        "Cluster 50 states by wildfire size class from 2001 to 2015 with the 2nd best number of clusters N\n",
        "\n",
        "Show cluster map \n",
        "\n",
        "'''\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# IMPLEMENT - 1 Point\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "cluster_map(df,kmeans_labels_for_cluster_M,\"2000 - 2015 Largest Wildfire Size Class\")\n",
        "\n",
        "cluster_map(df,kmeans_labels_for_cluster_N,\"2000 - 2015 Largest Wildfire Size Class\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "wIxj6Tiutgp7",
        "outputId": "44515fe7-327e-47ce-917b-c1560222ee9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://hazards.fema.gov/nri/Content/Images/StaticPageImages/map-wildfire_risk.png\" width=\"600\" height=\"300\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://hazards.fema.gov/nri/Content/Images/StaticPageImages/map-wildfire_risk.png\", width=600, height=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD3Rg01BTake"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "Q: FEMA provides a wildfire risk map (see above). Look back at your cluster maps. \n",
        "   Which one is the best match (visually) to the FEMA map? How many clusters?\n",
        "\n",
        "'''\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# QUESTION - 1 Point\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# Your answer."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('NumPy')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "03a2089fbed06f4aaa75051fc3fb7bbcaec74a1308c357da05fa87723e322cf7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
